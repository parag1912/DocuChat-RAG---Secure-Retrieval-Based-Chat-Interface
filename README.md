# ğŸ“˜ DocuChat-RAG: Secure Retrieval-Based Chat Interface

---

## ğŸ“‘ Table of Contents

1. [Introduction](#introduction)
2. [Project Overview](#project-overview)
3. [Architecture & Flow Diagram](#architecture--flow-diagram)
4. [Features](#features)
5. [Tech Stack](#tech-stack)
6. [Project Structure](#project-structure)
7. [Setup & Installation](#setup--installation)
8. [Configuration Guide](#configuration-guide)
9. [Phase-Wise Roadmap](#phase-wise-roadmap)
10. [Development Workflow](#development-workflow)
11. [Troubleshooting](#troubleshooting)
12. [License](#license)
13. [Acknowledgments](#acknowledgments)

---

## ğŸ“– Introduction

**DocuChat-RAG** is an intelligent, secure, and modular Q&A system powered by Retrieval-Augmented Generation (RAG). It enables organizations to build chatbots that can ingest custom documents and provide accurate, reference-backed answers using LLMs.

The system is designed with a focus on:
- Security (Local + API-based LLM options)
- Performance (Vector DBs, async chunking, incremental updates)
- Customizability (Pluggable pipelines, model & database options)

---

## ğŸŒŸ Project Overview

DocuChat-RAG enables document upload, chunking, vectorization, and querying via a chat interface. It supports async document ingestion, embedding, and real-time chat response using multiple LLMs (OpenAI, DeepSeek, Ollama).

Use Cases:
- Internal knowledge base search
- Policy document chatbots
- Privacy-first document QA tools

---

## ğŸ§  Architecture & Flow Diagram

The system is composed of modular services:

- **Frontend (Next.js)** for UI
- **Backend (FastAPI)** for APIs
- **Document Pipeline** for ingestion & embedding
- **Vector DBs (ChromaDB / Qdrant)** for retrieval
- **LLMs** for answer generation
- **MinIO** for document storage

### ğŸ”„ Flow Diagram

```mermaid
graph TB
    client["User"] --> upload["Upload Document"]
    upload --> docStore["MinIO: File Store"]
    upload --> asyncIngest["Async Processing"]
    asyncIngest --> extract["Text Extraction"] --> chunk["Text Chunking"] --> embed["Embedding"] --> storeVec["Vector DB"]

    client --> chatUI["Ask Question"] --> qEmbed["Query Embedding"] --> retrieve["Context Retrieval"]
    retrieve --> rerank["Re-ranking"] --> contextGen["Context Assembly"] --> llmCall["LLM API Call"] --> finalAnswer["Chat Response"]

    storeVec --> retrieve
```

---

## âœ¨ Features

- ğŸ“„ **Multi-format Document Upload** (PDF, DOCX, TXT, MD)
- âš™ï¸ **Asynchronous Chunking & Vectorization**
- ğŸ§  **Context-aware Chat with LLMs**
- ğŸ“Œ **Source Citation in Responses**
- ğŸ§© **Modular Architecture with Swappable Backends (LLM/DB)**
- ğŸ›¡ **Supports both Local and Cloud Models (OpenAI, Ollama)**
- ğŸ” **API-based Access with JWT & OAuth2 Auth**
- ğŸ§µ **Streaming Chat UI**
- ğŸ“Š **Admin Dashboard for Job Tracking, Logs, API Management**

---

## ğŸ§° Tech Stack

### ğŸ”§ Backend
- **FastAPI** â€“ REST API layer
- **LangChain** â€“ RAG logic wrapper
- **ChromaDB / Qdrant** â€“ Vector search engine
- **MinIO** â€“ Distributed object store
- **MySQL** â€“ Metadata storage
- **Python 3.9+**

### ğŸ’» Frontend
- **Next.js 14 (React)** â€“ UI framework
- **Tailwind CSS** â€“ Styling
- **Shadcn/UI** â€“ Component Library
- **TypeScript** â€“ Safety & structure
- **Vercel AI SDK** â€“ Streaming LLM integration

---

## ğŸ“ Project Structure

```bash
rag-web-ui/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/               # FastAPI routes
â”‚   â”œâ”€â”€ core/              # Config & settings
â”‚   â”œâ”€â”€ jobs/              # Async processing
â”‚   â”œâ”€â”€ services/          # Embedding, LLM, RAG
â”‚   â””â”€â”€ database/          # ORM models & DB connectors
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ (Next.js app)
â”œâ”€â”€ docker-compose.yml     # Multi-container deployment
â”œâ”€â”€ .env.example           # Sample config
â””â”€â”€ docs/
    â””â”€â”€ images/            # UI & diagrams
```

---

## ğŸ› ï¸ Setup & Installation

### ğŸ§¾ Prerequisites
- Docker & Docker Compose v2+
- Node.js 18+
- Python 3.9+
- 8GB+ RAM recommended

### ğŸš€ Getting Started
```bash
git clone https://github.com/rag-web-ui/rag-web-ui.git
cd rag-web-ui
cp .env.example .env
docker compose up -d --build
```

### ğŸŒ Access
- UI: http://127.0.0.1.nip.io
- API Docs: http://127.0.0.1.nip.io/redoc
- MinIO: http://127.0.0.1.nip.io:9001

---

## âš™ï¸ Configuration Guide

### Core
| Parameter                   | Description                | Required |
|----------------------------|----------------------------|----------|
| MYSQL_SERVER               | MySQL Server Address       | âœ…       |
| MYSQL_USER                 | MySQL Username             | âœ…       |
| MYSQL_PASSWORD             | MySQL Password             | âœ…       |
| MYSQL_DATABASE             | MySQL DB Name              | âœ…       |
| SECRET_KEY                 | JWT Secret                 | âœ…       |
| ACCESS_TOKEN_EXPIRE_MINUTES | Token expiry (min)       | âœ…       |

### LLM Setup
| Parameter        | Example              | Description         |
|------------------|----------------------|---------------------|
| CHAT_PROVIDER     | openai / ollama      | LLM Provider        |
| OPENAI_API_KEY    | sk-abc               | Key for OpenAI      |
| OLLAMA_MODEL      | llama2               | Local model name    |

### Embeddings
| Parameter                   | Example           | Description               |
|----------------------------|-------------------|---------------------------|
| EMBEDDINGS_PROVIDER        | openai            | Provider                  |
| OPENAI_EMBEDDINGS_MODEL    | text-embedding-ada-002 | OpenAI Embedding      |

---

## ğŸš§ Phase-Wise Roadmap

### âœ… Phase 1: MVP
- Document Upload + Chunking
- Basic Vector Search (Chroma)
- Chat UI + LLM Answering (OpenAI)

### ğŸš€ Phase 2: Enhancements
- Add Ollama + DeepSeek local model support
- Add Qdrant DB support
- Admin Dashboards & API key management

### ğŸ”œ Phase 3: AI Assistants
- Workflow Automation via NL Commands
- Multi-modal support (images)
- Fine-tuning via feedback loop

---

## ğŸ§ª Development Workflow

```bash
docker compose -f docker-compose.dev.yml up -d --build
```

1. Make changes in a feature branch
2. Run tests locally or via GitHub Actions
3. Push & Create Pull Request

### Guidelines
- Follow PEP8 for Python
- Follow Conventional Commits

---

## ğŸ§© Troubleshooting

Refer to the [Troubleshooting Guide](docs/troubleshooting.md) for help with:
- Docker build failures
- API issues
- Document parsing problems
- LLM API errors

---

## ğŸ“„ License

Licensed under [Apache 2.0](LICENSE)

---

## ğŸ™ Acknowledgments

Thanks to these open-source foundations:
- [FastAPI](https://fastapi.tiangolo.com/)
- [Langchain](https://python.langchain.com/)
- [ChromaDB](https://www.trychroma.com/)
- [Next.js](https://nextjs.org/)

---

<div align="center">
If this project helps you, please consider giving it a â­ï¸
</div>

